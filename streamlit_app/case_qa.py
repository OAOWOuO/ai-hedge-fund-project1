"""
Case Q&A â€” RAG chat grounded in course / case materials.

Answers questions using ONLY documents in data/raw/, with citations.
Supports uploading PDFs directly from the browser (no terminal needed).
Refuses to answer when the indexed documents don't support the question.
"""
from __future__ import annotations

import io
import os
import tempfile
from pathlib import Path

import streamlit as st

INDEX_DIR = Path("index")
DATA_RAW_DIR = Path("data/raw")
COLLECTION_NAME = "case_materials"
EMBED_MODEL = "text-embedding-3-small"
CHAT_MODEL = "gpt-4o-mini"
TOP_K = 5
DISTANCE_THRESHOLD = 1.0
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50

NOT_FOUND_MSG = (
    "Not enough information in the provided materials to answer this question. "
    "Please consult your course materials directly."
)

SYSTEM_PROMPT = (
    "You are a teaching assistant. Answer the question using ONLY the context "
    "provided below. Do not use any prior knowledge or information outside the "
    "context. If the context does not sufficiently support an answer, respond "
    f'with exactly: "{NOT_FOUND_MSG}"'
)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  chunking  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def chunk_text(text: str, source: str, page: int = 0) -> list[dict]:
    """Split text into overlapping word-window chunks."""
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        chunk_words = words[i: i + CHUNK_SIZE]
        chunk_text_str = " ".join(chunk_words)
        chunk_id = f"{source}_p{page}_c{i}"
        chunks.append({
            "text": chunk_text_str,
            "source": source,
            "page": page,
            "chunk_id": chunk_id,
        })
        if len(chunk_words) < CHUNK_SIZE:
            break
        i += CHUNK_SIZE - CHUNK_OVERLAP
    return chunks


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  indexing  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _build_index_from_bytes(file_bytes: bytes, filename: str, openai_client) -> int:
    """Extract text, chunk, embed, and store in ChromaDB. Returns chunk count."""
    import chromadb

    # Extract text
    all_chunks: list[dict] = []
    if filename.lower().endswith(".pdf"):
        try:
            from pypdf import PdfReader
            reader = PdfReader(io.BytesIO(file_bytes))
            for page_num, page in enumerate(reader.pages, start=1):
                page_text = page.extract_text() or ""
                if page_text.strip():
                    all_chunks.extend(chunk_text(page_text, filename, page_num))
        except Exception as e:
            st.error(f"Failed to read PDF '{filename}': {e}")
            return 0
    elif filename.lower().endswith((".md", ".txt")):
        text = file_bytes.decode("utf-8", errors="replace")
        all_chunks.extend(chunk_text(text, filename, 0))
    else:
        st.warning(f"Unsupported file type: '{filename}'. Only PDF, MD, and TXT are supported.")
        return 0

    if not all_chunks:
        st.warning(f"No text extracted from '{filename}'.")
        return 0

    # Embed in batches of 100
    INDEX_DIR.mkdir(parents=True, exist_ok=True)
    client = chromadb.PersistentClient(path=str(INDEX_DIR))
    try:
        collection = client.get_collection(COLLECTION_NAME)
    except Exception:
        collection = client.create_collection(
            COLLECTION_NAME,
            metadata={"hnsw:space": "cosine"},
        )

    batch_size = 100
    added = 0
    for i in range(0, len(all_chunks), batch_size):
        batch = all_chunks[i: i + batch_size]
        texts = [c["text"] for c in batch]
        try:
            resp = openai_client.embeddings.create(model=EMBED_MODEL, input=texts)
            embeddings = [r.embedding for r in resp.data]
        except Exception as e:
            st.error(f"Embedding failed: {e}")
            return added

        ids = [c["chunk_id"] for c in batch]
        metas = [{"source": c["source"], "page": c["page"], "chunk_id": c["chunk_id"]} for c in batch]

        # Skip duplicate IDs
        existing = set(collection.get(ids=ids)["ids"])
        new_idx = [j for j, cid in enumerate(ids) if cid not in existing]
        if new_idx:
            collection.add(
                ids=[ids[j] for j in new_idx],
                documents=[texts[j] for j in new_idx],
                embeddings=[embeddings[j] for j in new_idx],
                metadatas=[metas[j] for j in new_idx],
            )
            added += len(new_idx)

    return added


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  retrieval  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _get_collection():
    """Load ChromaDB collection; return None if index not yet built."""
    try:
        import chromadb
        client = chromadb.PersistentClient(path=str(INDEX_DIR))
        return client.get_collection(COLLECTION_NAME)
    except Exception:
        return None


def _embed(text: str, openai_client) -> list[float]:
    resp = openai_client.embeddings.create(model=EMBED_MODEL, input=text)
    return resp.data[0].embedding


def _retrieve(question: str, collection, openai_client) -> tuple[list[str], list[str]]:
    """Return (relevant_doc_texts, citation_strings)."""
    query_vec = _embed(question, openai_client)
    n = min(TOP_K, collection.count())
    if n == 0:
        return [], []

    results = collection.query(
        query_embeddings=[query_vec],
        n_results=n,
        include=["documents", "metadatas", "distances"],
    )
    docs = results["documents"][0]
    metas = results["metadatas"][0]
    dists = results["distances"][0]

    relevant_docs, citations = [], []
    for doc, meta, dist in zip(docs, metas, dists):
        if dist <= DISTANCE_THRESHOLD:
            relevant_docs.append(doc)
            source = meta.get("source", "unknown")
            page = meta.get("page", "?")
            chunk_id = meta.get("chunk_id", "?")
            citations.append(f"`{source}` â€” page {page} (chunk `{chunk_id}`)")

    return relevant_docs, citations


def _answer(question: str, context_docs: list[str], openai_client) -> str:
    if not context_docs:
        return NOT_FOUND_MSG
    context = "\n\n---\n\n".join(context_docs)
    completion = openai_client.chat.completions.create(
        model=CHAT_MODEL,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {question}"},
        ],
        temperature=0,
    )
    return completion.choices[0].message.content


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  UI  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def show_case_qa() -> None:
    # Back button
    col_back, _ = st.columns([1, 11])
    with col_back:
        if st.button("â† Back", key="back_caseqa"):
            st.session_state.current_view = "home"
            st.rerun()

    st.markdown("""
<div style="padding: 10px 0 10px 0;">
    <h2 style="color: #e6edf3;">ğŸ“š Case Q&amp;A</h2>
    <p style="color: #8b949e;">
        Ask questions about your course materials.
        Answers are grounded <em>only</em> in documents you upload here â€” with file &amp; page citations.
        Unsupported questions are refused explicitly.
    </p>
</div>
""", unsafe_allow_html=True)

    # â”€â”€ API KEY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    api_key = ""
    try:
        api_key = st.secrets.get("OPENAI_API_KEY", "")
    except Exception:
        pass
    if not api_key:
        api_key = os.getenv("OPENAI_API_KEY", "")

    if not api_key:
        st.error("**OPENAI_API_KEY** not set. Add it to your `.env` file or Streamlit secrets.")
        return

    import openai
    openai_client = openai.OpenAI(api_key=api_key)

    # â”€â”€ FILE UPLOADER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.write("### Upload Course Materials")
    st.caption(
        "Upload PDFs, Markdown, or text files. "
        "They will be indexed automatically â€” no terminal commands needed."
    )

    uploaded_files = st.file_uploader(
        "Drop files here or click to browse",
        type=["pdf", "md", "txt"],
        accept_multiple_files=True,
        key="caseqa_uploader",
    )

    if uploaded_files:
        files_to_index = []
        for uf in uploaded_files:
            # Track which files have already been indexed this session
            indexed_key = f"indexed_{uf.name}_{uf.size}"
            if not st.session_state.get(indexed_key):
                files_to_index.append(uf)

        if files_to_index:
            with st.spinner(f"Indexing {len(files_to_index)} file(s)â€¦ this may take a moment."):
                total_added = 0
                for uf in files_to_index:
                    file_bytes = uf.read()
                    n = _build_index_from_bytes(file_bytes, uf.name, openai_client)
                    total_added += n
                    indexed_key = f"indexed_{uf.name}_{uf.size}"
                    st.session_state[indexed_key] = True

            if total_added > 0:
                st.success(f"Indexed {total_added} new chunk(s) from {len(files_to_index)} file(s). Ready to chat!")
            else:
                st.info("No new chunks were added (files may already be indexed).")
        else:
            st.info(f"{len(uploaded_files)} file(s) already indexed this session.")

    # â”€â”€ CHAT INTERFACE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    collection = _get_collection()
    if collection is None or collection.count() == 0:
        st.divider()
        st.warning(
            "**No documents indexed yet.**\n\n"
            "Upload PDFs or text files above to get started."
        )
        return

    st.divider()
    st.caption(f"Index ready â€” {collection.count()} chunks from your uploaded documents.")

    # Render chat history
    if "qa_messages" not in st.session_state:
        st.session_state.qa_messages = []

    for msg in st.session_state.qa_messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # New question
    question = st.chat_input("Ask a question about the case materialsâ€¦")
    if question:
        st.session_state.qa_messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.markdown(question)

        with st.chat_message("assistant"):
            with st.spinner("Searching documentsâ€¦"):
                docs, citations = _retrieve(question, collection, openai_client)
                answer = _answer(question, docs, openai_client)

            response = answer
            if citations and NOT_FOUND_MSG not in answer:
                response += "\n\n**Sources:**\n" + "\n".join(f"- {c}" for c in citations)

            st.markdown(response)

        st.session_state.qa_messages.append({"role": "assistant", "content": response})
